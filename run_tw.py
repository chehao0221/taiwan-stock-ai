from __future__ import annotations

import os
import json
import warnings
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import Dict, List, Tuple

import pandas as pd
import requests
from xgboost import XGBRegressor
import pandas_market_calendars as mcal

from utils.market_calendar import is_market_open
from utils.safe_yfinance import safe_yf_download

warnings.filterwarnings("ignore")

# -----------------------------
# Basic settings
# -----------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_DIR = os.path.join(BASE_DIR, ".cache")
os.makedirs(CACHE_DIR, exist_ok=True)

HISTORY_FILE = os.path.join(BASE_DIR, "tw_history.csv")
TOP300_CACHE_FILE = os.path.join(CACHE_DIR, "top300_tw.json")

WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL", "").strip()

# ä½ å¸¸çœ‹çš„æ¬Šå€¼è‚¡ï¼ˆä¿ç•™ï¼‰
FIXED = ["2330.TW", "2317.TW", "2454.TW", "0050.TW", "2308.TW", "2382.TW"]

# -----------------------------
# Helpers
# -----------------------------
def _today_tw() -> str:
    """å°åŒ—æ™‚é–“ä»Šå¤© YYYY-MM-DD"""
    return datetime.now(ZoneInfo("Asia/Taipei")).strftime("%Y-%m-%d")


def pre_check() -> bool:
    # åªè¦ä¸æ˜¯äº¤æ˜“æ—¥å°±ä¸è·‘ï¼ˆé¿å…ç„¡æ„ç¾©çš„æŠ“è³‡æ–™/è¨“ç·´ï¼‰
    if not is_market_open("TW"):
        print("ğŸ“Œ ä»Šæ—¥éäº¤æ˜“æ—¥ï¼ˆå°è‚¡ä¼‘å¸‚ï¼‰")
        return False
    return True


def calc_pivot(df: pd.DataFrame) -> Tuple[float, float]:
    """è¿‘ 20 æ—¥ Pivot æ”¯æ’/å£“åŠ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    r = df.iloc[-20:]
    h, l, c = float(r["High"].max()), float(r["Low"].min()), float(r["Close"].iloc[-1])
    p = (h + l + c) / 3
    sup = round(2 * p - h, 1)
    res = round(2 * p - l, 1)
    return sup, res


def nth_trading_day_after(start_date: str, n: int, calendar_name: str = "XTAI") -> str:
    """
    å›å‚³ start_date ä¹‹å¾Œç¬¬ n å€‹äº¤æ˜“æ—¥ï¼ˆä¸å« start_date ç•¶å¤©ï¼‰ã€‚
    ä½¿ç”¨ pandas_market_calendars çš„äº¤æ˜“æ‰€æ—¥æ›†é¿å…é€±æœ«/å‡æ—¥ã€‚
    """
    cal = mcal.get_calendar(calendar_name)
    # å–è¼ƒå¯¬é¬†çš„å€é–“ï¼Œé¿å…é‡åˆ°é•·å‡
    schedule = cal.schedule(start_date=start_date, end_date=pd.Timestamp(start_date) + pd.Timedelta(days=60))
    days = schedule.index.strftime("%Y-%m-%d").tolist()
    if start_date in days:
        pos = days.index(start_date)
        target = pos + n
    else:
        # è‹¥ start_date ä¸æ˜¯äº¤æ˜“æ—¥ï¼ˆç†è«–ä¸Šä¸æœƒç™¼ç”Ÿï¼Œå› ç‚º pre_check å·²æ“‹ï¼‰
        # å°±æ‰¾ä¸‹ä¸€å€‹äº¤æ˜“æ—¥ç•¶ä½œèµ·é»
        target = n - 1
    if target >= len(days):
        raise RuntimeError("äº¤æ˜“æ—¥æ›†ä¸è¶³ï¼Œè«‹åŠ å¤§ end_date ç¯„åœ")
    return days[target]


def _read_history() -> pd.DataFrame:
    if not os.path.exists(HISTORY_FILE):
        return pd.DataFrame(
            columns=[
                "run_date",
                "ticker",
                "pred",
                "price_at_run",
                "sup",
                "res",
                "settle_date",
                "settle_close",
                "realized_return",
                "hit",
                "status",
                "updated_at",
            ]
        )

    df = pd.read_csv(HISTORY_FILE)

    # ä¿è­‰æ¬„ä½å®Œæ•´ï¼ˆé¿å…èˆŠæª”æ¡ˆï¼‰
    for col in ["settle_close", "realized_return", "hit", "status", "updated_at"]:
        if col not in df.columns:
            df[col] = pd.NA

    if "status" not in df.columns:
        df["status"] = "pending"

    df["status"] = df["status"].fillna("pending")
    return df


def _write_history(df: pd.DataFrame) -> None:
    df.to_csv(HISTORY_FILE, index=False, encoding="utf-8-sig")


def _load_top300_cache(today: str) -> List[str] | None:
    try:
        with open(TOP300_CACHE_FILE, "r", encoding="utf-8") as f:
            obj = json.load(f)
        if obj.get("date") == today and isinstance(obj.get("tickers"), list):
            return obj["tickers"]
    except Exception:
        pass
    return None


def _save_top300_cache(today: str, tickers: List[str]) -> None:
    try:
        with open(TOP300_CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump({"date": today, "tickers": tickers}, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


def get_top300_by_volume(today: str) -> List[str]:
    """
    å…ˆæŠ“ã€Œä¸Šå¸‚è‚¡ç¥¨æ¸…å–®ã€â†’ ç”¨ yfinance æ‹‰ 1M è³‡æ–™è¨ˆç®—è¿‘ 20 æ—¥å¹³å‡é‡ â†’ å–å‰ 300ã€‚
    é€™ä¸€æ­¥æœ€å®¹æ˜“è¢« yfinance 429ï¼Œæ‰€ä»¥åšã€Œç•¶æ—¥å¿«å–ã€ï¼šåŒä¸€å¤©å…§é‡è·‘ä¸æœƒå†æŠ“ä¸€è¼ªã€‚
    """
    cached = _load_top300_cache(today)
    if cached:
        return cached

    # å–å¾—ä¸Šå¸‚æ¸…å–®ï¼ˆå››ç¢¼ï¼‰
    url = "https://isin.twse.com.tw/isin/C_public.jsp?strMode=2"
    html = requests.get(url, timeout=15).text
    df = pd.read_html(html)[0]
    df.columns = df.iloc[0]
    codes = df.iloc[1:][df.columns[0]].astype(str).str.split(" ").str[0]
    tickers = [f"{c}.TW" for c in codes if c.isdigit() and len(c) == 4]

    # åˆ†æ‰¹æŠ“å–è¿‘ 1 å€‹æœˆè³‡æ–™ï¼Œç®—å‡é‡
    data = safe_yf_download(tickers, period="1mo", max_chunk=80)
    avg_vol: Dict[str, float] = {}
    for t, d in data.items():
        if d is None or len(d) < 5:
            continue
        v = float(d["Volume"].tail(20).mean())
        if pd.notna(v) and v > 0:
            avg_vol[t] = v

    top300 = sorted(avg_vol, key=avg_vol.get, reverse=True)[:300]
    _save_top300_cache(today, top300)
    return top300


def settle_history(today: str) -> Tuple[pd.DataFrame, str]:
    """
    å° tw_history.csv è£¡ã€Œå·²åˆ°æœŸï¼ˆsettle_date <= todayï¼‰ã€ä½†å°šæœªçµç®—çš„é …ç›®åšçµç®—ã€‚
    å›å‚³ï¼ˆæ›´æ–°å¾Œçš„ df, çµç®—è¨Šæ¯æ–‡å­—ï¼‰
    """
    hist = _read_history()
    if hist.empty:
        return hist, ""

    # æ‰¾å¾…çµç®—ä¸”åˆ°æœŸçš„
    pending = hist[(hist["status"] == "pending") & (hist["settle_date"].astype(str) <= today)]
    if pending.empty:
        return hist, ""

    tickers = sorted(pending["ticker"].astype(str).unique().tolist())
    # ç‚ºäº†ç©©å®šï¼Œç›´æ¥æŠ“ 3moï¼Œè¶³å¤ æ¶µè“‹ settle_date
    data = safe_yf_download(tickers, period="3mo", max_chunk=60)

    settled_rows = []
    for idx, row in pending.iterrows():
        t = str(row["ticker"])
        settle_date = str(row["settle_date"])
        d = data.get(t)
        if d is None or d.empty:
            continue

        # yfinance index å¯èƒ½æ˜¯ Timestampï¼ˆå«æ™‚å€/ä¸å«æ™‚å€ï¼‰ï¼Œçµ±ä¸€è½‰å­—ä¸²æ—¥æœŸ
        d2 = d.copy()
        d2.index = pd.to_datetime(d2.index).strftime("%Y-%m-%d")

        if settle_date not in d2.index:
            # ä¿å®ˆï¼šå¦‚æœæ‰¾ä¸åˆ°ï¼Œå…ˆç•¥éï¼ˆä¸äº‚è£œæ—¥æœŸï¼‰
            continue

        settle_close = float(d2.loc[settle_date, "Close"])
        price_at_run = float(row["price_at_run"])
        rr = (settle_close / price_at_run) - 1.0

        hist.at[idx, "settle_close"] = round(settle_close, 2)
        hist.at[idx, "realized_return"] = rr
        hist.at[idx, "hit"] = int(rr > 0)
        hist.at[idx, "status"] = "settled"
        hist.at[idx, "updated_at"] = datetime.now(ZoneInfo("Asia/Taipei")).strftime("%Y-%m-%d %H:%M:%S")
        settled_rows.append((t, settle_date, rr))

    if not settled_rows:
        return hist, ""

    # çµç®—æ‘˜è¦ï¼ˆåªåˆ—æœ€å¤š 8 ç­†ï¼Œé¿å… Discord å¤ªé•·ï¼‰
    lines = ["ğŸ å°è‚¡ 5 æ—¥å›æ¸¬çµç®—ï¼ˆåˆ°æœŸé …ç›®ï¼‰"]
    for t, dte, rr in settled_rows[:8]:
        lines.append(f"- {t} @ {dte}: `{rr:+.2%}`")
    if len(settled_rows) > 8:
        lines.append(f"... å¦å¤–é‚„æœ‰ {len(settled_rows) - 8} ç­†å·²çµç®—")

    return hist, "\n".join(lines) + "\n"


def append_today_predictions(hist: pd.DataFrame, today: str, rows: List[dict]) -> pd.DataFrame:
    if not rows:
        return hist

    now_str = datetime.now(ZoneInfo("Asia/Taipei")).strftime("%Y-%m-%d %H:%M:%S")
    df_new = pd.DataFrame(rows)
    df_new["run_date"] = today
    df_new["status"] = "pending"
    df_new["updated_at"] = now_str

    # é¿å…åŒä¸€å¤©é‡è·‘æŠŠåŒä¸€æª”é‡è¤‡å¯«å…¥ï¼šç”¨ (run_date, ticker) å»é‡
    if not hist.empty:
        hist["run_date"] = hist["run_date"].astype(str)
        hist["ticker"] = hist["ticker"].astype(str)
        existing = set(zip(hist["run_date"], hist["ticker"]))
        df_new = df_new[~df_new.apply(lambda r: (today, r["ticker"]) in existing, axis=1)]

    if df_new.empty:
        return hist

    out = pd.concat([hist, df_new], ignore_index=True)
    return out


# -----------------------------
# Main
# -----------------------------
def run() -> None:
    today = _today_tw()

    # 1) å…ˆåšæ­·å²çµç®—ï¼ˆåˆ°æœŸçš„å°±è£œä¸Šï¼‰
    hist, settle_msg = settle_history(today)

    # 2) ä»Šæ—¥é æ¸¬ï¼ˆTop300 + FIXEDï¼‰
    universe = list(dict.fromkeys(FIXED + get_top300_by_volume(today)))
    data = safe_yf_download(universe, period="2y", max_chunk=60)

    feats = ["mom20", "bias", "vol_ratio"]
    results: Dict[str, dict] = {}

    for s, df in data.items():
        if df is None or len(df) < 160:
            continue

        df = df.copy()
        df["mom20"] = df["Close"].pct_change(20)
        ma20 = df["Close"].rolling(20).mean()
        df["bias"] = (df["Close"] - ma20) / ma20
        df["vol_ratio"] = df["Volume"] / df["Volume"].rolling(20).mean()
        df["target"] = df["Close"].shift(-5) / df["Close"] - 1

        df = df.dropna()
        if len(df) < 120:
            continue

        train = df.iloc[:-1]

        model = XGBRegressor(
            n_estimators=90,
            max_depth=3,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
        )
        model.fit(train[feats], train["target"])

        pred = float(model.predict(df[feats].iloc[-1:])[0])
        sup, res = calc_pivot(df)

        results[s] = {
            "pred": pred,
            "price": round(float(df["Close"].iloc[-1]), 2),
            "sup": sup,
            "res": res,
        }

    if not results:
        msg = "âš ï¸ ä»Šæ—¥ç„¡å¯ç”¨çµæœï¼ˆå¯èƒ½è³‡æ–™ä¸è¶³æˆ–æŠ“å–å¤±æ•—ï¼‰"
        _post(msg)
        return

    top = sorted(results.items(), key=lambda kv: kv[1]["pred"], reverse=True)[:5]

    # 3) å¯«å…¥æ­·å²ï¼ˆä»Šæ—¥ Top5ï¼‰
    new_rows = []
    for t, r in top:
        settle_date = nth_trading_day_after(today, 5, calendar_name="XTAI")
        new_rows.append(
            {
                "ticker": t,
                "pred": r["pred"],
                "price_at_run": r["price"],
                "sup": r["sup"],
                "res": r["res"],
                "settle_date": settle_date,
                "settle_close": pd.NA,
                "realized_return": pd.NA,
                "hit": pd.NA,
            }
        )
    hist = append_today_predictions(hist, today, new_rows)
    _write_history(hist)

    # 4) çµ„ Discord è¨Šæ¯
    msg = f"ğŸ“ˆ å°è‚¡æ”¶ç›¤ AI åˆ†æï¼ˆ{today}ï¼‰\n\n"
    msg += "ğŸ”¥ é ä¼° 5 æ—¥å ±é…¬ Top 5\n\n"
    for t, r in top:
        msg += f"{t}: é ä¼° `{r['pred']:+.2%}`\n"
        msg += f" â”” ç¾åƒ¹: `{r['price']}` (Pivot æ”¯æ’: `{r['sup']}` / å£“åŠ›: `{r['res']}`)\n"

    if settle_msg:
        msg += "\n\n" + settle_msg

    msg += "\nğŸ’¡ AI ç‚ºæ©Ÿç‡æ¨¡å‹ï¼Œåƒ…ä¾›ç ”ç©¶åƒè€ƒ"

    _post(msg[:1900])


def _post(content: str) -> None:
    if WEBHOOK_URL:
        try:
            requests.post(WEBHOOK_URL, json={"content": content}, timeout=15)
        except Exception as e:
            print(f"âš ï¸ Discord ç™¼é€å¤±æ•—: {e}")
            print(content)
    else:
        print(content)


if __name__ == "__main__":
    if pre_check():
        run()
