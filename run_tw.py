from __future__ import annotations

import os
import json
import warnings
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import Dict, List, Tuple

import pandas as pd
import requests
from xgboost import XGBRegressor
import pandas_market_calendars as mcal

from utils.market_calendar import is_market_open
from utils.safe_yfinance import safe_yf_download

warnings.filterwarnings("ignore")

# -----------------------------
# Basic settings
# -----------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_DIR = os.path.join(BASE_DIR, ".cache")
os.makedirs(CACHE_DIR, exist_ok=True)

HISTORY_FILE = os.path.join(BASE_DIR, "tw_history.csv")
TOP300_CACHE_FILE = os.path.join(CACHE_DIR, "top300_tw.json")

WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL", "").strip()

# æ¬Šå€¼è‚¡ï¼ˆä¿ç•™ä½ åŸæœ¬é‚£å€‹æ¦‚å¿µï¼‰
FIXED = ["2330.TW", "2317.TW", "2454.TW", "0050.TW", "2308.TW", "2382.TW"]

# -----------------------------
# Helpers
# -----------------------------
def _now_tw() -> datetime:
    return datetime.now(ZoneInfo("Asia/Taipei"))


def _today_tw() -> str:
    return _now_tw().strftime("%Y-%m-%d")


def pre_check() -> bool:
    # åªè¦ä¸æ˜¯äº¤æ˜“æ—¥å°±ä¸è·‘ï¼ˆé¿å…ç„¡æ„ç¾©çš„æŠ“è³‡æ–™/è¨“ç·´ï¼‰
    if not is_market_open("TW"):
        print("ğŸ“Œ ä»Šæ—¥éäº¤æ˜“æ—¥ï¼ˆå°è‚¡ä¼‘å¸‚ï¼‰")
        return False
    return True


def calc_pivot(df: pd.DataFrame) -> Tuple[float, float]:
    """è¿‘ 20 æ—¥ Pivot æ”¯æ’/å£“åŠ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    r = df.iloc[-20:]
    h, l, c = float(r["High"].max()), float(r["Low"].min()), float(r["Close"].iloc[-1])
    p = (h + l + c) / 3
    sup = round(2 * p - h, 1)
    res = round(2 * p - l, 1)
    return sup, res


def nth_trading_day_after(start_date: str, n: int, calendar_name: str = "XTAI") -> str:
    """
    å›å‚³ start_date ä¹‹å¾Œç¬¬ n å€‹äº¤æ˜“æ—¥ï¼ˆä¸å« start_date ç•¶å¤©ï¼‰
    ç”¨äº¤æ˜“æ‰€æ—¥æ›†é¿é–‹é€±æœ«/å‡æ—¥
    """
    cal = mcal.get_calendar(calendar_name)
    schedule = cal.schedule(
        start_date=start_date,
        end_date=pd.Timestamp(start_date) + pd.Timedelta(days=60)  # é¿å…é‡åˆ°é•·å‡ä¸å¤ ç”¨
    )
    days = schedule.index.strftime("%Y-%m-%d").tolist()

    if start_date in days:
        pos = days.index(start_date)
        target = pos + n
    else:
        # ç†è«–ä¸Šä¸æœƒç™¼ç”Ÿï¼ˆå› ç‚º pre_check å·²æ“‹éäº¤æ˜“æ—¥ï¼‰
        target = n - 1

    if target >= len(days):
        raise RuntimeError("äº¤æ˜“æ—¥æ›†ä¸è¶³ï¼Œè«‹åŠ å¤§ end_date ç¯„åœ")
    return days[target]


def _read_history() -> pd.DataFrame:
    """
    è®€å– tw_history.csvï¼ˆæ–°æ ¼å¼ï¼‰
    è‹¥ä¸å­˜åœ¨ï¼šå»ºç«‹ç©ºè¡¨ï¼ˆåŒ…å«å®Œæ•´æ¬„ä½ï¼‰
    """
    cols = [
        "run_date",
        "ticker",
        "pred",
        "price_at_run",
        "sup",
        "res",
        "settle_date",
        "settle_close",
        "realized_return",
        "hit",
        "status",
        "updated_at",
    ]

    if not os.path.exists(HISTORY_FILE):
        return pd.DataFrame(columns=cols)

    df = pd.read_csv(HISTORY_FILE)

    # è£œé½Šç¼ºæ¬„ä½ï¼ˆå°±ç®—ä½ ä¹‹å¾Œåˆæ›æ ¼å¼ä¹Ÿä¸æœƒç‚¸ï¼‰
    for c in cols:
        if c not in df.columns:
            df[c] = pd.NA

    df["status"] = df["status"].fillna("pending")
    df["run_date"] = df["run_date"].astype(str)
    df["ticker"] = df["ticker"].astype(str)
    df["settle_date"] = df["settle_date"].fillna("").astype(str)
    return df


def _write_history(df: pd.DataFrame) -> None:
    df.to_csv(HISTORY_FILE, index=False, encoding="utf-8-sig")


def _load_top300_cache(today: str) -> List[str] | None:
    try:
        with open(TOP300_CACHE_FILE, "r", encoding="utf-8") as f:
            obj = json.load(f)
        if obj.get("date") == today and isinstance(obj.get("tickers"), list):
            return obj["tickers"]
    except Exception:
        pass
    return None


def _save_top300_cache(today: str, tickers: List[str]) -> None:
    try:
        with open(TOP300_CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump({"date": today, "tickers": tickers}, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


def get_top300_by_volume(today: str) -> List[str]:
    """
    å…ˆæŠ“ã€Œä¸Šå¸‚è‚¡ç¥¨æ¸…å–®ã€â†’ ç”¨ yfinance æ‹‰ 1M è³‡æ–™è¨ˆç®—è¿‘ 20 æ—¥å¹³å‡é‡ â†’ å–å‰ 300
    é€™ä¸€æ­¥æœ€å®¹æ˜“è¢« yfinance 429ï¼Œæ‰€ä»¥åšã€Œç•¶æ—¥å¿«å–ã€ï¼šåŒä¸€å¤©å…§é‡è·‘ä¸æœƒå†æŠ“ä¸€è¼ª
    """
    cached = _load_top300_cache(today)
    if cached:
        return cached

    url = "https://isin.twse.com.tw/isin/C_public.jsp?strMode=2"
    html = requests.get(url, timeout=15).text
    table = pd.read_html(html)[0]
    table.columns = table.iloc[0]
    codes = table.iloc[1:][table.columns[0]].astype(str).str.split(" ").str[0]
    tickers = [f"{c}.TW" for c in codes if c.isdigit() and len(c) == 4]

    data = safe_yf_download(tickers, period="1mo", max_chunk=80)
    avg_vol: Dict[str, float] = {}
    for t, d in data.items():
        if d is None or len(d) < 5:
            continue
        v = float(d["Volume"].tail(20).mean())
        if pd.notna(v) and v > 0:
            avg_vol[t] = v

    top300 = sorted(avg_vol, key=avg_vol.get, reverse=True)[:300]
    _save_top300_cache(today, top300)
    return top300


def settle_history(today: str) -> Tuple[pd.DataFrame, str]:
    """
    çµç®— tw_history.csv è£¡ï¼š
    - status == pending
    - settle_date <= today
    ä¸¦è¼¸å‡ºä½ åŸæœ¬æƒ³è¦çš„ã€Œé ä¼° vs å¯¦éš› + âœ…/âŒã€æ ¼å¼
    """
    hist = _read_history()
    if hist.empty:
        return hist, ""

    if "settle_date" not in hist.columns:
        return hist, ""

    # settle_date å…¨ç©ºå°±ä¸ç”¨çµç®—
    if hist["settle_date"].astype(str).str.len().eq(0).all():
        return hist, ""

    pending = hist[
        (hist["status"].astype(str) == "pending")
        & (hist["settle_date"].astype(str) <= today)
        & (hist["settle_date"].astype(str).str.len() > 0)
    ]

    if pending.empty:
        return hist, ""

    tickers = sorted(pending["ticker"].astype(str).unique().tolist())
    data = safe_yf_download(tickers, period="3mo", max_chunk=60)

    settled_lines = []
    now_str = _now_tw().strftime("%Y-%m-%d %H:%M:%S")

    for idx, row in pending.iterrows():
        t = str(row["ticker"])
        settle_date = str(row["settle_date"])

        d = data.get(t)
        if d is None or d.empty:
            continue

        d2 = d.copy()
        d2.index = pd.to_datetime(d2.index).strftime("%Y-%m-%d")

        if settle_date not in d2.index:
            continue

        settle_close = float(d2.loc[settle_date, "Close"])
        price_at_run = float(row["price_at_run"])
        rr = (settle_close / price_at_run) - 1.0

        pred = row.get("pred", pd.NA)
        try:
            pred_f = float(pred)
        except Exception:
            pred_f = None

        hit = int(rr > 0)
        mark = "âœ…" if hit == 1 else "âŒ"

        hist.at[idx, "settle_close"] = round(settle_close, 2)
        hist.at[idx, "realized_return"] = rr
        hist.at[idx, "hit"] = hit
        hist.at[idx, "status"] = "settled"
        hist.at[idx, "updated_at"] = now_str

        if pred_f is None:
            settled_lines.append(f"â€¢ {t}: å¯¦éš› {rr:+.2%} {mark}")
        else:
            settled_lines.append(f"â€¢ {t}: é ä¼° {pred_f:+.2%} | å¯¦éš› {rr:+.2%} {mark}")

    if not settled_lines:
        return hist, ""

    # ä¿æŒä½ åŸæœ¬çš„é¡¯ç¤ºæ–¹å¼ï¼ˆæ¨™é¡Œ + é»åˆ—ï¼‰
    msg = "ğŸ 5 æ—¥å›æ¸¬çµç®—å ±å‘Š\n" + "\n".join(settled_lines[:10])
    if len(settled_lines) > 10:
        msg += f"\nâ€¦ å¦å¤–é‚„æœ‰ {len(settled_lines) - 10} ç­†å·²çµç®—"

    return hist, msg


def append_today_predictions(hist: pd.DataFrame, today: str, rows: List[dict]) -> pd.DataFrame:
    if not rows:
        return hist

    now_str = _now_tw().strftime("%Y-%m-%d %H:%M:%S")
    df_new = pd.DataFrame(rows)
    df_new["run_date"] = today
    df_new["status"] = "pending"
    df_new["updated_at"] = now_str

    # é¿å…åŒä¸€å¤©é‡è·‘é‡è¤‡å¯«å…¥ï¼ˆrun_date + tickerï¼‰
    if not hist.empty:
        hist["run_date"] = hist["run_date"].astype(str)
        hist["ticker"] = hist["ticker"].astype(str)
        existing = set(zip(hist["run_date"], hist["ticker"]))
        df_new = df_new[~df_new.apply(lambda r: (today, str(r["ticker"])) in existing, axis=1)]

    if df_new.empty:
        return hist

    return pd.concat([hist, df_new], ignore_index=True)


def _post(content: str) -> None:
    if WEBHOOK_URL:
        try:
            requests.post(WEBHOOK_URL, json={"content": content}, timeout=15)
        except Exception as e:
            print(f"âš ï¸ Discord ç™¼é€å¤±æ•—: {e}")
            print(content)
    else:
        print(content)


# -----------------------------
# Main
# -----------------------------
def run() -> None:
    today = _today_tw()

    # 1) å…ˆåšæ­·å²çµç®—ï¼ˆåˆ°æœŸçš„è£œä¸Š âœ…/âŒï¼‰
    hist, settle_msg = settle_history(today)

    # 2) ä»Šæ—¥é æ¸¬ï¼ˆTop300 + æ¬Šå€¼ï¼‰
    universe = list(dict.fromkeys(FIXED + get_top300_by_volume(today)))
    data = safe_yf_download(universe, period="2y", max_chunk=60)

    feats = ["mom20", "bias", "vol_ratio"]
    results: Dict[str, dict] = {}

    for s, df in data.items():
        if df is None or len(df) < 160:
            continue

        df = df.copy()
        df["mom20"] = df["Close"].pct_change(20)
        ma20 = df["Close"].rolling(20).mean()
        df["bias"] = (df["Close"] - ma20) / ma20
        df["vol_ratio"] = df["Volume"] / df["Volume"].rolling(20).mean()
        df["target"] = df["Close"].shift(-5) / df["Close"] - 1

        df = df.dropna()
        if len(df) < 120:
            continue

        train = df.iloc[:-1]

        model = XGBRegressor(
            n_estimators=90,
            max_depth=3,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
        )
        model.fit(train[feats], train["target"])

        pred = float(model.predict(df[feats].iloc[-1:])[0])
        sup, res = calc_pivot(df)

        results[s] = {
            "pred": pred,
            "price": round(float(df["Close"].iloc[-1]), 2),
            "sup": sup,
            "res": res,
        }

    if not results:
        _post("âš ï¸ ä»Šæ—¥ç„¡å¯ç”¨çµæœï¼ˆå¯èƒ½è³‡æ–™ä¸è¶³æˆ–æŠ“å–å¤±æ•—ï¼‰")
        return

    # ä½ åŸæœ¬æƒ³è¦ã€Œæµ·é¸ Top5ï¼ˆé»‘é¦¬ï¼‰ã€çš„æ¨£å­
    top = sorted(results.items(), key=lambda kv: kv[1]["pred"], reverse=True)[:5]

    # 3) å¯«å…¥æ­·å²ï¼ˆä»Šæ—¥ Top5ï¼Œä¸¦è¨ˆç®—ç¬¬ 5 å€‹äº¤æ˜“æ—¥çµç®—æ—¥ï¼‰
    new_rows = []
    for t, r in top:
        settle_date = nth_trading_day_after(today, 5, calendar_name="XTAI")
        new_rows.append(
            {
                "ticker": t,
                "pred": r["pred"],
                "price_at_run": r["price"],
                "sup": r["sup"],
                "res": r["res"],
                "settle_date": settle_date,
                "settle_close": pd.NA,
                "realized_return": pd.NA,
                "hit": pd.NA,
            }
        )

    hist = append_today_predictions(hist, today, new_rows)
    _write_history(hist)

    # 4) Discord é¡¯ç¤ºï¼šç¶­æŒä½  README é‚£ç¨®é¢¨æ ¼
    msg = f"ğŸ“Š å°è‚¡ AI é€²éšé æ¸¬å ±å‘Š ({today})\n\n"
    msg += "ğŸ† AI æµ·é¸ Top 5 (æ½›åŠ›é»‘é¦¬)\n"
    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£"]

    for i, (t, r) in enumerate(top):
        medal = medals[i] if i < len(medals) else "â€¢"
        msg += f"{medal} {t}: é ä¼° {r['pred']:+.2%}\n"
        msg += f"   â”” ç¾åƒ¹: {r['price']} / æ”¯æ’: {r['sup']} / å£“åŠ›: {r['res']}\n"

    if settle_msg:
        msg += "\n" + settle_msg + "\n"

    msg += "\nâš ï¸ å…è²¬è²æ˜ï¼šæœ¬å°ˆæ¡ˆåƒ…ä¾›ç ”ç©¶åƒè€ƒï¼Œä¸æ§‹æˆä»»ä½•æŠ•è³‡å»ºè­°ã€‚"

    _post(msg[:1900])


if __name__ == "__main__":
    if pre_check():
        run()
